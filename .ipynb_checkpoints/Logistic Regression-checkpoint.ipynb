{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prequisite - Preprocessing Data\n",
    "\n",
    "Perform the following steps before trying the exercises:\n",
    "\n",
    "a) Import pandas as \"pd\" and load the lab1 dataset into \"df\"\n",
    "\n",
    "b) Print dataset information to refresh your memory\n",
    "\n",
    "c) Run preprocess_data function on the dataframe to perform preprocessing steps \n",
    "discussed last week\n",
    "\n",
    "d) Split your data into training and test with 70:30 distribution, stratified, random state 0.\n",
    "\n",
    "================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Import pandas as \"pd\" and load the lab1 dataset into \"df\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('lab1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Print dataset information to refresh your memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TargetB</th>\n",
       "      <th>ID</th>\n",
       "      <th>TargetD</th>\n",
       "      <th>GiftCnt36</th>\n",
       "      <th>GiftCntAll</th>\n",
       "      <th>GiftCntCard36</th>\n",
       "      <th>GiftCntCardAll</th>\n",
       "      <th>GiftAvgLast</th>\n",
       "      <th>GiftAvg36</th>\n",
       "      <th>GiftAvgAll</th>\n",
       "      <th>...</th>\n",
       "      <th>PromCntCardAll</th>\n",
       "      <th>StatusCat96NK</th>\n",
       "      <th>StatusCatStarAll</th>\n",
       "      <th>DemCluster</th>\n",
       "      <th>DemAge</th>\n",
       "      <th>DemGender</th>\n",
       "      <th>DemHomeOwner</th>\n",
       "      <th>DemMedHomeValue</th>\n",
       "      <th>DemPctVeterans</th>\n",
       "      <th>DemMedIncome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>14974</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.50</td>\n",
       "      <td>9.25</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>6294</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>15.88</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>67.0</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>186800</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>46110</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.17</td>\n",
       "      <td>3.73</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>87600</td>\n",
       "      <td>36</td>\n",
       "      <td>38750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>185937</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.67</td>\n",
       "      <td>8.50</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>139200</td>\n",
       "      <td>27</td>\n",
       "      <td>38942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>29637</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>53.0</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>168100</td>\n",
       "      <td>37</td>\n",
       "      <td>71509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TargetB      ID  TargetD  GiftCnt36  GiftCntAll  GiftCntCard36  \\\n",
       "0        0   14974      NaN          2           4              1   \n",
       "1        0    6294      NaN          1           8              0   \n",
       "2        1   46110      4.0          6          41              3   \n",
       "3        1  185937     10.0          3          12              3   \n",
       "4        0   29637      NaN          1           1              1   \n",
       "\n",
       "   GiftCntCardAll  GiftAvgLast  GiftAvg36  GiftAvgAll  ...  PromCntCardAll  \\\n",
       "0               3         17.0      13.50        9.25  ...              13   \n",
       "1               3         20.0      20.00       15.88  ...              24   \n",
       "2              20          6.0       5.17        3.73  ...              22   \n",
       "3               8         10.0       8.67        8.50  ...              16   \n",
       "4               1         20.0      20.00       20.00  ...               6   \n",
       "\n",
       "   StatusCat96NK  StatusCatStarAll  DemCluster  DemAge  DemGender  \\\n",
       "0              A                 0           0     NaN          F   \n",
       "1              A                 0          23    67.0          F   \n",
       "2              S                 1           0     NaN          M   \n",
       "3              E                 1           0     NaN          M   \n",
       "4              F                 0          35    53.0          M   \n",
       "\n",
       "   DemHomeOwner  DemMedHomeValue  DemPctVeterans DemMedIncome  \n",
       "0             U                0               0            0  \n",
       "1             U           186800              85            0  \n",
       "2             U            87600              36        38750  \n",
       "3             U           139200              27        38942  \n",
       "4             U           168100              37        71509  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9686 entries, 0 to 9685\n",
      "Data columns (total 28 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   TargetB           9686 non-null   int64  \n",
      " 1   ID                9686 non-null   int64  \n",
      " 2   TargetD           4843 non-null   float64\n",
      " 3   GiftCnt36         9686 non-null   int64  \n",
      " 4   GiftCntAll        9686 non-null   int64  \n",
      " 5   GiftCntCard36     9686 non-null   int64  \n",
      " 6   GiftCntCardAll    9686 non-null   int64  \n",
      " 7   GiftAvgLast       9686 non-null   float64\n",
      " 8   GiftAvg36         9686 non-null   float64\n",
      " 9   GiftAvgAll        9686 non-null   float64\n",
      " 10  GiftAvgCard36     7906 non-null   float64\n",
      " 11  GiftTimeLast      9686 non-null   int64  \n",
      " 12  GiftTimeFirst     9686 non-null   int64  \n",
      " 13  PromCnt12         9686 non-null   int64  \n",
      " 14  PromCnt36         9686 non-null   int64  \n",
      " 15  PromCntAll        9686 non-null   int64  \n",
      " 16  PromCntCard12     9686 non-null   int64  \n",
      " 17  PromCntCard36     9686 non-null   int64  \n",
      " 18  PromCntCardAll    9686 non-null   int64  \n",
      " 19  StatusCat96NK     9686 non-null   object \n",
      " 20  StatusCatStarAll  9686 non-null   int64  \n",
      " 21  DemCluster        9686 non-null   int64  \n",
      " 22  DemAge            7279 non-null   float64\n",
      " 23  DemGender         9686 non-null   object \n",
      " 24  DemHomeOwner      9686 non-null   object \n",
      " 25  DemMedHomeValue   9686 non-null   int64  \n",
      " 26  DemPctVeterans    9686 non-null   int64  \n",
      " 27  DemMedIncome      9686 non-null   int64  \n",
      "dtypes: float64(6), int64(19), object(3)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Run preprocess_data function on the dataframe to perform preprocessing steps discussed last week "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary variable TargetD and ID \n",
    "df.drop(['ID', 'TargetD'], axis=1, inplace=True)\n",
    "    \n",
    "# Change DemCluster from integer to nominal/str\n",
    "df['DemCluster'] = df['DemCluster'].astype(str)\n",
    "\n",
    "# Change DemHomeOwner into binary variable (Integer Encoding)\n",
    "dem_home_owner_map = {'U':0, 'H': 1}\n",
    "df['DemHomeOwner'] = df['DemHomeOwner'].map(dem_home_owner_map)\n",
    "\n",
    "# Denote errorneous values in DemMidIncome\n",
    "mask = df['DemMedIncome'] < 1   # filter out any DemMedIncome with value less than 1\n",
    "df.loc[mask, 'DemMedIncome'] = np.nan  # denote zeroes in DemMedIncome as missing now\n",
    "\n",
    "# Impute missing values DemAge, DemMedIncome and GiftAveCard36 with mean value\n",
    "df['DemAge'] = df['DemAge'].fillna(value=df['DemAge'].mean(), inplace=True)\n",
    "df['DemMedIncome'] = df['DemMedIncome'].fillna(value=df['DemMedIncome'].mean(), inplace=True)\n",
    "df['GiftAvgCard36'].fillna(df['GiftAvgCard36'].mean(), inplace=True)  # different syntax\n",
    "\n",
    "# Machine learning algorithms cannot work with categorical data directly because sklearn models only accept numerical matrices as input.\n",
    "# Change data from categorical variables to binary variables. Categorical data must be converted to numbers.\n",
    "# This converting process is commonly referred as one-hot encoding. \n",
    "# We do one hot encoding using .get_dummies().\n",
    "df['DemGender'] = df['DemGender'].astype('category')  \n",
    "\n",
    "# one hot encoding all categorical variables\n",
    "# all numerical variables are automatically excluded\n",
    "\n",
    "# one hot encoding\n",
    "df = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Split your data into training and test with 70:30 distribution, stratified, random state 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df['TargetB']\n",
    "X = df.drop(['TargetB'], axis=1)\n",
    "\n",
    "# Setting random state = 0\n",
    "rs = 0\n",
    "\n",
    "# Training set = 70%\n",
    "# Test Set = 30%\n",
    "# Stratify = Yes\n",
    "X_mat = np.asmatrix(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.3, \n",
    "                                                    stratify=y, random_state=rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Standardisation and Logistic Regression\n",
    "\n",
    "Perform following operations and answer the following questions:\n",
    "\n",
    "a) What is the difference between logistic regression and linear regression?  \n",
    "\n",
    "b) Describe how logistic regression perform its prediction. \n",
    "\n",
    "c) Write code to perform standardisation on your training and test dataset. \n",
    "\n",
    "d) What does standardisation do to your data? How does it benefit your regression model? \n",
    "\n",
    "e) Write code to fit a logistic regression model to your training data. How does it perform on the training and test data? Do you see any indication of overfitting?  \n",
    "\n",
    "f) Write code to find the most important features in your model.\n",
    "\n",
    "================================================================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) What is the difference between logistic regression and linear regression?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Linear Regression is a regression algorithm while Logistic Regression is a classification algorithm for machine learning.\n",
    "\n",
    "2) Linear Regression resolve the problem of predicting/estimating the output value for a given element X (say f(x)). The result of the prediction is a continuous function where the values may be positive or negative. For example, predicting children's height given their age, weight, and other factors. In general, it is about fitting a straight line in the data \n",
    "\n",
    "3) Logistic Regression resolve classification problems where given an element that we have to classify. The outcome of dependent variable is discrete (not continuous). For example, given a mail to classify whether it as spam or not. Generally, Logistic Regression have binary target variables, but there can be two more categories of target variables that can be predicted by it. It is about fitting a curve to the data. \n",
    "\n",
    "4) Linear regression gives an equation which is of the form Y = mX + C, means equation with degree 1. However, logistic regression gives an equation which is of the form Y = 1 / (1 + e^-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic 1.PNG\" width=\"800\" height=\"200\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Describe how logistic regression perform its prediction. \n",
    "\n",
    "Logistic Regression is a supervised learning classification algorithm used to predict the probability of a target variable. The nature of target or dependent variable is dichotomous, which means there would be only two possible classes. \n",
    "\n",
    "It do not require a linear relationship between the dependent and independent variables. The outcome is discrete and not continuous. The dependent variable is binary in nature having data coded as either 1 (stands for success/yes) or 0 (stands for failure/no).\n",
    "\n",
    "The Logistic Regression algorithm learns the 'weight' associated with each feature. The model will try to minimize the cost function, which basically says how far off the current predictions to the ground truth.\n",
    "\n",
    "Assumption in Logistic Regression:\n",
    "a) In case of binary logistic regression, the target variables must be binary and the desired outcome is represented by the factor level 1.\n",
    "\n",
    "b) There should not be any multi-collinearity in the model, which means the independent variables must be independent of each other.\n",
    "\n",
    "c) We must include meaningful variables in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Write code to perform standardisation on your training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train, y_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) What does standardisation do to your data? How does it benefit your regression model?\n",
    "\n",
    "Standardization is the process of putting different variables on the same scale. It means these variables originally do not give equal contribution to the analysis. Hence, it is required to transform the data to comparable scales. The idea is to rescale an original variable to have equal range and/or variance.\n",
    "\n",
    "Regression models are sensitive to extreme or outlying values in the input space. To avoid this problem, we should scale our inputs first before building our logistic regression model. In sklearn, this can easily be done using StandardScaler.\n",
    "\n",
    "Standardizing independent variables can also help us to determine which variables are the most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e)(i) Write code to fit a logistic regression model to your training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.5859882005899705 \n",
      "Test accuracy: 0.5763936682725396\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.58      0.58      1453\n",
      "           1       0.58      0.57      0.57      1453\n",
      "\n",
      "    accuracy                           0.58      2906\n",
      "   macro avg       0.58      0.58      0.58      2906\n",
      "weighted avg       0.58      0.58      0.58      2906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", model.score(X_train, y_train),\n",
    "      \"\\nTest accuracy:\", model.score(X_test, y_test))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e)(ii) How does it perform on the training and test data? Do you see any indication of overfitting?  \n",
    "\n",
    "\n",
    "Overfitting occurs if a model performs well on training data but does not well on test data. In this case, the performance of the model on training and testing datasets has no significance difference hence the model is not overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f) Write code to find the most important features in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive coefficient means positive change in the input feature will have positive correlation to the prediction value. \n",
    "\n",
    "Negative coefficient does the reverse. \n",
    "\n",
    "Using largest absolute coefficients to give us the clues of which variables are important. I sorting the absolute coefficient in descending order and limit to 20 for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GiftCnt36 : 0.07439448463481192\n",
      "GiftCntAll : -0.016924292563642366\n",
      "GiftCntCard36 : 0.10663312417862549\n",
      "GiftCntCardAll : -0.025465675047160373\n",
      "GiftAvgLast : -0.02845281190648024\n",
      "GiftAvg36 : -0.032996633990946676\n",
      "GiftAvgAll : 0.02932758922770193\n",
      "GiftAvgCard36 : -0.045924192614627844\n",
      "GiftTimeLast : -0.19003922054478953\n",
      "GiftTimeFirst : 0.3076543735487482\n",
      "PromCnt12 : -0.1198626983812884\n",
      "PromCnt36 : 0.09004725480280376\n",
      "PromCntAll : 0.12455561361580184\n",
      "PromCntCard12 : 0.013220043918111607\n",
      "PromCntCard36 : 0.09998473570690312\n",
      "PromCntCardAll : -0.3658332240396197\n",
      "StatusCatStarAll : 0.06520836152519284\n",
      "DemHomeOwner : 0.02801469683666773\n",
      "DemMedHomeValue : 0.08500853358859292\n",
      "DemPctVeterans : 0.005757326410809153\n",
      "StatusCat96NK_A : -0.03118081218104886\n",
      "StatusCat96NK_E : 0.06750649878169673\n",
      "StatusCat96NK_F : 0.007897533415741297\n",
      "StatusCat96NK_L : 0.016240227060780047\n",
      "StatusCat96NK_N : 0.023675630046334484\n",
      "StatusCat96NK_S : -0.009117654009009948\n",
      "DemCluster_0 : 0.05864622808416183\n",
      "DemCluster_1 : 0.024767257866875978\n",
      "DemCluster_10 : -0.054192682098912204\n",
      "DemCluster_11 : 0.013193533170185222\n",
      "DemCluster_12 : 0.0017098342554826712\n",
      "DemCluster_13 : 0.016749627113141194\n",
      "DemCluster_14 : 0.012225421598972358\n",
      "DemCluster_15 : -0.0353954450290739\n",
      "DemCluster_16 : -0.04617406365282512\n",
      "DemCluster_17 : 0.006712385021051927\n",
      "DemCluster_18 : 0.015027022557835093\n",
      "DemCluster_19 : -0.010006533807026437\n",
      "DemCluster_2 : -0.026259831250992818\n",
      "DemCluster_20 : 0.029288122078267108\n",
      "DemCluster_21 : -0.004826276877918263\n",
      "DemCluster_22 : 0.0026623692863903003\n",
      "DemCluster_23 : 0.04251784184728244\n",
      "DemCluster_24 : 0.027588039125151717\n",
      "DemCluster_25 : 0.015109983037920353\n",
      "DemCluster_26 : 0.00655658423757833\n",
      "DemCluster_27 : -0.003813802238450757\n",
      "DemCluster_28 : 0.024059416023915193\n",
      "DemCluster_29 : 0.020631011135930716\n",
      "DemCluster_3 : 0.035228109921959266\n",
      "DemCluster_30 : -0.05243976728990517\n",
      "DemCluster_31 : 0.02126345507015878\n",
      "DemCluster_32 : -0.05022873601776453\n",
      "DemCluster_33 : -0.014003485767638022\n",
      "DemCluster_34 : 0.015625370390732928\n",
      "DemCluster_35 : -0.004467657271012959\n",
      "DemCluster_36 : -0.01506063034232773\n",
      "DemCluster_37 : -0.010375667147291047\n",
      "DemCluster_38 : 0.002186052734200996\n",
      "DemCluster_39 : 0.017406656150809174\n",
      "DemCluster_4 : 0.02661118374106079\n",
      "DemCluster_40 : 0.0462735130596608\n",
      "DemCluster_41 : -0.019604135134787582\n",
      "DemCluster_42 : 0.01732219148137525\n",
      "DemCluster_43 : -0.014745673754843596\n",
      "DemCluster_44 : -0.05853839559582289\n",
      "DemCluster_45 : -0.027465794725334086\n",
      "DemCluster_46 : -0.016550112321600488\n",
      "DemCluster_47 : -0.034891378674273194\n",
      "DemCluster_48 : -0.03461980352071132\n",
      "DemCluster_49 : -0.020286631686610396\n",
      "DemCluster_5 : -0.013693995958883504\n",
      "DemCluster_50 : 0.026529358738923193\n",
      "DemCluster_51 : -0.005121469472988865\n",
      "DemCluster_52 : -0.005922045715164691\n",
      "DemCluster_53 : 0.03461603878014644\n",
      "DemCluster_6 : 0.006038617200565755\n",
      "DemCluster_7 : 0.029338892333242193\n",
      "DemCluster_8 : -0.02851446718802582\n",
      "DemCluster_9 : -0.014747571144772818\n",
      "DemGender_F : 0.0008709560934063297\n",
      "DemGender_M : -0.0010976804770559943\n",
      "DemGender_U : 0.00044761196919445835\n"
     ]
    }
   ],
   "source": [
    "# First, print out all feature name associated with each coefficient to see all the coefficient number\n",
    "\n",
    "feature_names = X.columns\n",
    "coef = model.coef_[0]\n",
    "\n",
    "coef = coef[:]\n",
    "for i in range(len(coef)):\n",
    "    print(feature_names[i], ':', coef[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PromCntCardAll : -0.3658332240396197\n",
      "GiftTimeFirst : 0.3076543735487482\n",
      "GiftTimeLast : -0.19003922054478953\n",
      "PromCntAll : 0.12455561361580184\n",
      "PromCnt12 : -0.1198626983812884\n",
      "GiftCntCard36 : 0.10663312417862549\n",
      "PromCntCard36 : 0.09998473570690312\n",
      "PromCnt36 : 0.09004725480280376\n",
      "DemMedHomeValue : 0.08500853358859292\n",
      "GiftCnt36 : 0.07439448463481192\n",
      "StatusCat96NK_E : 0.06750649878169673\n",
      "StatusCatStarAll : 0.06520836152519284\n",
      "DemCluster_0 : 0.05864622808416183\n",
      "DemCluster_44 : -0.05853839559582289\n",
      "DemCluster_10 : -0.054192682098912204\n",
      "DemCluster_30 : -0.05243976728990517\n",
      "DemCluster_32 : -0.05022873601776453\n",
      "DemCluster_40 : 0.0462735130596608\n",
      "DemCluster_16 : -0.04617406365282512\n",
      "GiftAvgCard36 : -0.045924192614627844\n"
     ]
    }
   ],
   "source": [
    "# Then, sort features in descending order based on absolute coefficient number\n",
    "indices = np.argsort(np.absolute(coef))\n",
    "indices = np.flip(indices, axis=0)\n",
    "\n",
    "# 20 most important features\n",
    "indices = indices[:20]\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', coef[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above 20 features are the most important features in this model. However this only applies to regression on standardised features. Without standardisation, each feature can have different scales, where coefficients do not help in interpreting the impact a feature has on the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
